{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/image-clustering-implementation-with-pytorch-587af1d14123\n",
    "# 日本語訳:  https://ichi.pro/pytorch-o-shiyoshita-gazo-kurasutaringu-no-jisso-97285066277155\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "class EncoderVGG(nn.Module):\n",
    "    '''Encoder of image based on the architecture of VGG-16 with batch normalization.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = 3\n",
    "    channels_code = 512\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(EncoderVGG, self).__init__()\n",
    "\n",
    "        vgg = models.vgg16_bn(pretrained=pretrained_params)\n",
    "        del vgg.classifier\n",
    "        del vgg.avgpool\n",
    "\n",
    "        self.encoder = self._encodify_(vgg)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "overhead-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "class EncoderVGG(nn.Module):\n",
    "    channels_in = 3\n",
    "    channels_code = 512\n",
    "    \n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(EncoderVGG, self).__init__()\n",
    "        \n",
    "        vgg = models.vgg16_bn(pretrained=pretrained_params)\n",
    "        del vgg.classifier\n",
    "        del vgg.avgpool\n",
    "        \n",
    "        self.encoder = self._encodify_(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def _encodify_(self, encoder):\n",
    "        '''Create list of modules for encoder based on the architecture in VGG template model.\n",
    "        In the encoder-decoder architecture, the unpooling operations in the decoder require pooling\n",
    "        indices from the corresponding pooling operation in the encoder. In VGG template, these indices\n",
    "        are not returned. Hence the need for this method to extent the pooling operations.\n",
    "        Args:\n",
    "            encoder : the template VGG model\n",
    "        Returns:\n",
    "            modules : the list of modules that define the encoder corresponding to the VGG model\n",
    "        '''\n",
    "        modules = nn.ModuleList()\n",
    "        for module in encoder.features:\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                module_add = nn.MaxPool2d(kernel_size=module.kernel_size,\n",
    "                                          stride=module.stride,\n",
    "                                          padding=module.padding,\n",
    "                                          return_indices=True)\n",
    "                modules.append(module_add)\n",
    "            else:\n",
    "                modules.append(module)\n",
    "\n",
    "        return modules\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interior-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encodify_(self, encoder):\n",
    "    modules = nn.ModuleList()\n",
    "    for module in encoder.features:\n",
    "        if isinstance(module, nn.MaxPool2d):      # isinstance 関数は 1 番目の引数に指定したオブジェクトが 2 番目の引数に指定したデータ型と等しいかどうかを返します\n",
    "            module_add = nn.MaxPool2d(kerbel_size=module.kernel_size,\n",
    "                                  stride=module.stride,\n",
    "                                  padding=module.padding,\n",
    "                                  return_indices=True)       # デフォルトのVGGではindiceが返ってこないので、ここ重要\n",
    "            modules.append(module_add)\n",
    "        else:\n",
    "            modules.append(module)\n",
    "        \n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def forward(self, x):\n",
    "        '''Execute the encoder on the image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_code (Tensor): code tensor\n",
    "            pool_indices (list): Pool indices tensors in order of the pooling modules\n",
    "        '''\n",
    "        pool_indices = []\n",
    "        x_current = x\n",
    "        for module_encode in self.encoder:\n",
    "            output = module_encode(x_current)\n",
    "\n",
    "            # If the module is pooling, there are two outputs, the second the pool indices\n",
    "            if isinstance(output, tuple) and len(output) == 2:\n",
    "                x_current = output[0]\n",
    "                pool_indices.append(output[1])\n",
    "            else:\n",
    "                x_current = output\n",
    "\n",
    "        return x_current, pool_indices\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    pool_indices = []\n",
    "    x_current = x\n",
    "    for module_encode in self.encoder:\n",
    "        output = module_encode(x_current)\n",
    "        \n",
    "        # If the module is pooling, there are two outputs, the second the pool indices\n",
    "        if isinstance(output, tuple) and len(output) == 2:\n",
    "            x_current = output[0]\n",
    "            pool_indices.append(output[1])\n",
    "        else:\n",
    "            x_current = output\n",
    "            \n",
    "    return x_current, pool_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next the Decoder.\n",
    "\"\"\"\n",
    "class DecoderVGG(nn.Module):\n",
    "    '''Decoder of code based on the architecture of VGG-16 with batch normalization.\n",
    "    Args:\n",
    "        encoder: The encoder instance of `EncoderVGG` that is to be inverted into a decoder\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_code\n",
    "    channels_out = 3\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super(DecoderVGG, self).__init__()\n",
    "\n",
    "        self.decoder = self._invert_(encoder)\n",
    "        \n",
    "    def _invert_(self, encoder):\n",
    "        '''Invert the encoder in order to create the decoder as a (more or less) mirror image of the encoder\n",
    "        The decoder is comprised of two principal types: the 2D transpose convolution and the 2D unpooling. The 2D transpose\n",
    "        convolution is followed by batch normalization and activation. Therefore as the module list of the encoder\n",
    "        is iterated over in reverse, a convolution in encoder is turned into transposed convolution plus normalization\n",
    "        and activation, and a maxpooling in encoder is turned into unpooling.\n",
    "        Args:\n",
    "            encoder (ModuleList): the encoder\n",
    "        Returns:\n",
    "            decoder (ModuleList): the decoder obtained by \"inversion\" of encoder\n",
    "        '''\n",
    "        modules_transpose = []\n",
    "        for module in reversed(encoder):\n",
    "\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                kwargs = {'in_channels' : module.out_channels, 'out_channels' : module.in_channels,\n",
    "                          'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.ConvTranspose2d(**kwargs)\n",
    "                module_norm = nn.BatchNorm2d(module.in_channels)\n",
    "                module_act = nn.ReLU(inplace=True)\n",
    "                modules_transpose += [module_transpose, module_norm, module_act]\n",
    "\n",
    "            elif isinstance(module, nn.MaxPool2d):\n",
    "                kwargs = {'kernel_size' : module.kernel_size, 'stride' : module.stride,\n",
    "                          'padding' : module.padding}\n",
    "                module_transpose = nn.MaxUnpool2d(**kwargs)\n",
    "                modules_transpose += [module_transpose]\n",
    "\n",
    "        # Discard the final normalization and activation, so final module is convolution with bias\n",
    "        modules_transpose = modules_transpose[:-2]\n",
    "\n",
    "        return nn.ModuleList(modules_transpose)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "impressed-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderVGG(nn.Module):\n",
    "    channels_in = EncoderVGG.channels_in\n",
    "    channels_out = 3\n",
    "    \n",
    "    def __init__(self, encoder):\n",
    "        super(DecoderVGG, self).__init__()\n",
    "        self.decoder = self._invert_(encoder)\n",
    "        \n",
    "    def _invert_(self, encoder):\n",
    "        modules_transpose = []\n",
    "        for module in reversed(encoder):\n",
    "            \n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                kwargs = {'in_channels':module.out_channels, 'out_channels':module.in_channels,\n",
    "                          'kernel_size':module.kernel_size, 'stride':module.stride,\n",
    "                          'padding':module.padding}\n",
    "                module_transpose = nn.MaxUnpool2d(**kwargs)\n",
    "                module_norm = nn.BatchNorm2d(module.in_channels)\n",
    "                module_act = nn.ReLU(inplace=True)\n",
    "                modules_transpose += [module_transpose, module_norm, module_act]\n",
    "                \n",
    "            elif isinstance(module, nn.MaxPool2d):\n",
    "                kwargs = {'kernel_size':module.kernel_size, 'stride':module.stride,\n",
    "                          'padding':module.padding}\n",
    "                module_transpose = nn.MaxUnpool2d(**kwargs)\n",
    "                modules_transpose += [module_transpose]\n",
    "                \n",
    "        # Discard the final normalization and activation, so final module is convolution with bias\n",
    "        modules_transpose = modules_transpose[:-2]\n",
    "        \n",
    "        return nn.ModuleList(modules_transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " def forward(self, x, pool_indices):\n",
    "        '''Execute the decoder on the code tensor input\n",
    "        Args:\n",
    "            x (Tensor): code tensor obtained from encoder\n",
    "            pool_indices (list): Pool indices Pytorch tensors in order the pooling modules in the encoder\n",
    "        Returns:\n",
    "            x (Tensor): decoded image tensor\n",
    "        '''\n",
    "        x_current = x\n",
    "\n",
    "        k_pool = 0\n",
    "        reversed_pool_indices = list(reversed(pool_indices))\n",
    "        for module_decode in self.decoder:\n",
    "\n",
    "            # If the module is unpooling, collect the appropriate pooling indices\n",
    "            if isinstance(module_decode, nn.MaxUnpool2d):\n",
    "                x_current = module_decode(x_current, indices=reversed_pool_indices[k_pool])\n",
    "                k_pool += 1\n",
    "            else:\n",
    "                x_current = module_decode(x_current)\n",
    "\n",
    "        return x_current\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "superb-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, pool_indices):\n",
    "    x_current = x\n",
    "    \n",
    "    k_pool = 0\n",
    "    reversed_pool_indices = list(reversed(pool_indices))         # reversed(): 順序を逆にする\n",
    "    for modul_decode in self.decoder:\n",
    "        if isinstance(module_decode, nn.MaxUnpool2d):\n",
    "            x_current = module_decode(x_current, indices=reversed_pool_indices[k_pool])\n",
    "            k_pool += 1\n",
    "        else:\n",
    "            x_current = module_decode(x_current)\n",
    "            \n",
    "    return x_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class AutoEncoderVGG(nn.Module):\n",
    "    '''Auto-Encoder based on the VGG-16 with batch normalization template model. The class is comprised of\n",
    "    an encoder and a decoder.\n",
    "    Args:\n",
    "        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.\n",
    "            Defaults to True.\n",
    "    '''\n",
    "    channels_in = EncoderVGG.channels_in\n",
    "    channels_code = EncoderVGG.channels_code\n",
    "    channels_out = DecoderVGG.channels_out\n",
    "\n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(AutoEncoderVGG, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderVGG(pretrained_params=pretrained_params)\n",
    "        self.decoder = DecoderVGG(self.encoder.encoder)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward the autoencoder for image input\n",
    "        Args:\n",
    "            x (Tensor): image tensor\n",
    "        Returns:\n",
    "            x_prime (Tensor): image tensor following encoding and decoding\n",
    "        '''\n",
    "        code, pool_indices = self.encoder(x)\n",
    "        x_prime = self.decoder(code, pool_indices)\n",
    "\n",
    "        return x_prime\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "realistic-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderVGG(nn.Module):\n",
    "    channels_in = EncoderVGG.channels_in\n",
    "    channels_code = EncoderVGG.channels_code\n",
    "    channels_out = DecoderVGG.channels_out\n",
    "    \n",
    "    def __init__(self, pretrained_params=True):\n",
    "        super(AutoEncoderVGG, self).__init__()\n",
    "        \n",
    "        self.encoder = EncoderVGG(pretrained_params=pretarained_params)\n",
    "        self.decoder = DecoderVGG(self.encoder.encoder)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        code, pool_indices = self.encoder(x)\n",
    "        x_prime = self.decoder(code, pool_indices)\n",
    "        \n",
    "        return x_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "\n",
    "class LocalAggregationLoss(nn.Module):\n",
    "    '''Local Aggregation Loss module from \"Local Aggregation for Unsupervised Learning of Visual Embeddings\" by\n",
    "    Zhuang, Zhai and Yamins (2019), arXiv:1903.12355v2\n",
    "    '''\n",
    "    def __init__(self, temperature,\n",
    "                 k_nearest_neighbours, clustering_repeats, number_of_centroids,\n",
    "                 memory_bank,\n",
    "                 kmeans_n_init=1, nn_metric=cosine_distance, nn_metric_params={}):\n",
    "        super(LocalAggregationLoss, self).__init__()\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.memory_bank = memory_bank\n",
    "\n",
    "        self.neighbour_finder = NearestNeighbors(n_neighbors=k_nearest_neighbours + 1,\n",
    "                                                 algorithm='ball_tree',\n",
    "                                                 metric=nn_metric, metric_params=nn_metric_params)\n",
    "        self.clusterer = []\n",
    "        for k_clusterer in range(clustering_repeats):\n",
    "            self.clusterer.append(KMeans(n_clusters=number_of_centroids,\n",
    "                                         init='random', n_init=kmeans_n_init))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "theoretical-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors       #  k近傍法(k-Nearest Neighbour)\n",
    "from sklearn.cluster import KMeans      # K-Means 法: 教師なし学習におけるクラスタリング法\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "\n",
    "class LocalAggregationLoss(nn.Module):\n",
    "    def __init__(self, temperature,\n",
    "                 k_nearest_neighbours, clustering_repeats, number_of_centroids,\n",
    "                 memory_bank,\n",
    "                 kmeans_n_init=1, nn_metric=cosine_distance, nn_metric_params={}):\n",
    "        super(LocalAggregationLoss, self).__init__()\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.memory_bank = memory_bank\n",
    "        \n",
    "        self.neighbour_finder = NearestNeighbors(n_neighbors=k_nearest_neighbours + 1,\n",
    "                                                 algorithm='ball_tree',\n",
    "                                                 metric=nn_metric, metric_params=nn_metric_params)\n",
    "        self.clusterer = []\n",
    "        for k_clusterer in range(clustering_repeats):\n",
    "            self.clusterer.append(KMeans(n_clusters=number_of_centroids,\n",
    "                                         init='random', n_init=kmeans_n_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def forward(self, codes, indices):\n",
    "        '''Forward pass for the local aggregation loss module'''\n",
    "        assert codes.shape[0] == len(indices)\n",
    "\n",
    "        codes = codes.type(torch.DoubleTensor)\n",
    "        code_data = normalize(codes.detach().numpy(), axis=1)\n",
    "\n",
    "        # Compute and collect arrays of indices that define the constants in the loss function. Note that\n",
    "        # no gradients are computed for these data values in backward pass\n",
    "        self.memory_bank.update_memory(code_data, indices)\n",
    "        \n",
    "        background_neighbours = self._nearest_neighbours(code_data, indices)\n",
    "        close_neighbours = self._close_grouper(indices)\n",
    "        neighbour_intersect = self._intersecter(background_neighbours, close_neighbours)\n",
    "\n",
    "        # Compute the probability density for the codes given the constants of the memory bank\n",
    "        v = F.normalize(codes, p=2, dim=1)\n",
    "        d1 = self._prob_density(v, background_neighbours)\n",
    "        d2 = self._prob_density(v, neighbour_intersect)\n",
    "        \n",
    "        return torch.sum(torch.log(d1) - torch.log(d2)) / codes.shape[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adult-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, codes, indices):\n",
    "    assert codes.shape[0] == len(indices)\n",
    "    \n",
    "    codes = codes.type(torch.DoubleTensor)\n",
    "    code_data = normalize(codes.detach().numpy(), axis=1)\n",
    "    \n",
    "    self.memory_bank.update_memory(code_data, indices)\n",
    "    \n",
    "    background_neighbours = self._nearest_neighbours(code_data, indices)\n",
    "    close_neighbours = self._close_grouper(indices)\n",
    "    neighbour_intersect = self._intersecter(background_neighbours, close_neighbours)\n",
    "    \n",
    "    v = F.normalize(codes, p=2, dim=1)\n",
    "    d1 = self._prob_density(v, background_neighbours)\n",
    "    d2 = self._prob_density(v, neighbour_intersect)\n",
    "    \n",
    "    return torch.sum(torch.log(d1) - torch.log(d2)) / codes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class MemoryBank(object):\n",
    "    '''Memory bank\n",
    "    Args:\n",
    "        n_vectors (int): Number of vectors the memory bank should hold\n",
    "        dim_vector (int): Dimension of the vectors the memory bank should hold\n",
    "        memory_mixing_rate (float, optional): Fraction of new vector to add to currently stored vector. The value\n",
    "            should be between 0.0 and 1.0, the greater the value the more rapid the update. The mixing rate can be\n",
    "            set during calling `update_memory`.\n",
    "    '''\n",
    "    def __init__(self, n_vectors, dim_vector, memory_mixing_rate):\n",
    "\n",
    "        self.dim_vector = dim_vector\n",
    "        self.vectors = np.array([marsaglia(dim_vector) for _ in range(n_vectors)])\n",
    "        self.memory_mixing_rate = memory_mixing_rate\n",
    "        self.mask_init = np.array([False] * n_vectors)\n",
    "\n",
    "    def update_memory(self, vectors, index):\n",
    "        '''Update the memory with new vectors'''\n",
    "        if isinstance(index, int):\n",
    "            self.vectors[index] = self._update_(vectors, self.vectors[index])\n",
    "\n",
    "        elif isinstance(index, np.ndarray):\n",
    "            for ind, vector in zip(index, vectors):\n",
    "                self.vectors[ind] = self._update_(vector, self.vectors[ind])\n",
    "\n",
    "    def mask(self, inds_int):\n",
    "        '''Construct a Boolean mask given integer indices'''\n",
    "        ret_mask = []\n",
    "        for row in inds_int:\n",
    "            row_mask = np.full(self.vectors.shape[0], False)\n",
    "            row_mask[row.astype(int)] = True\n",
    "            ret_mask.append(row_mask)\n",
    "\n",
    "        return np.array(ret_mask)\n",
    "\n",
    "    def _update_(self, vector_new, vector_recall):\n",
    "        return vector_new * self.memory_mixing_rate + vector_recall * (1.0 - self.memory_mixing_rate)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ordered-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank(object):\n",
    "    def __init__(self, n_vectors, dim_vector, memory_mixing_rate):\n",
    "        \n",
    "        self.dim_vector = dim_vector\n",
    "        self.vectors = np.array([marsaglia(dim_vector) for _ in range(n_vectors)])      #Marsagliaの方法 で検索\n",
    "        self.memory_mixing_rate = memory_mixing_rate\n",
    "        self.mask_init = np.array([False] * n_vectors)\n",
    "        \n",
    "    def update_memory(self, vectors, index):\n",
    "        if isinstance(index, int):\n",
    "            self.vectors[index] = self._update_(vectors, self.vectors[index])\n",
    "            \n",
    "        elif isinstance(index, np.ndarray):\n",
    "            for ind, vector in zip(index, vectors):\n",
    "                self.vectors[ind] = self._update_(vector, self.vectors[ind])\n",
    "                \n",
    "    def mask(self, inds_int):\n",
    "        ret_mask = []\n",
    "        for row in inds_int:\n",
    "            row_mask = np.full(self.vectors.shape[0], False)\n",
    "            row_mask[row.astype(int)] = True\n",
    "            ret_mask.append(row_mask)\n",
    "            \n",
    "        return np.array(ret_mask)\n",
    "    \n",
    "    def _update_(self, vector_new, vector_recall):\n",
    "        return vector_new * self.memory_mixing_rate + vector_recall * (1.0 - self.memory_mixing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def _nearest_neighbours(self, codes_data, indices):\n",
    "        '''Ascertain indices in memory bank of the k-nearest neighbours to given codes\n",
    "        \n",
    "        Returns:\n",
    "            indices_nearest (numpy.ndarray): Boolean array of k-nearest neighbours for the batch of codes\n",
    "        '''\n",
    "        self.neighbour_finder.fit(self.memory_bank.vectors)\n",
    "        indices_nearest = self.neighbour_finder.kneighbors(codes_data, return_distance=False)\n",
    "\n",
    "        return self.memory_bank.mask(indices_nearest)\n",
    "\n",
    "    def _close_grouper(self, indices):\n",
    "        '''Ascertain indices in memory bank of vectors that are in the same cluster as vectors of given indices\n",
    "        Returns:\n",
    "            indices_close (numpy.ndarray): Boolean array of close neighbours for the batch of codes\n",
    "        '''\n",
    "        memberships = [[]] * len(indices)\n",
    "        for clusterer in self.clusterer:\n",
    "            clusterer.fit(self.memory_bank.vectors)\n",
    "            for k_index, cluster_index in enumerate(clusterer.labels_[indices]):\n",
    "                other_members = np.where(clusterer.labels_ == cluster_index)[0]\n",
    "                other_members_union = np.union1d(memberships[k_index], other_members)\n",
    "                memberships[k_index] = other_members_union.astype(int)\n",
    "\n",
    "        return self.memory_bank.mask(np.array(memberships, dtype=object))\n",
    "\n",
    "    def _intersecter(self, n1, n2):\n",
    "        '''Compute set intersection of two boolean arrays'''\n",
    "        return np.array([[v1 and v2 for v1, v2 in zip(n1_x, n2_x)] for n1_x, n2_x in zip(n1, n2)])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "wired-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nearest_neighbours(self, codes_data, indices):\n",
    "    self.neighbour_finder.fit(self.memory_bank.vectors)\n",
    "    indices_nearest = self.neighbour_finder.kneighbors(codes_data, return_distance=False)\n",
    "    \n",
    "    return self.memory_bank.mask(indices_nearest)\n",
    "\n",
    "    def _close_grouper(self, indices):\n",
    "        memberships = [[]]  * len(indices)\n",
    "        for clusterer in self.clusterer:\n",
    "            clusterer.fit(self.memory_bank.vectors)\n",
    "            for k_index, cluster_index in enumerate(clusterer.labels_[indices]):\n",
    "                other_members = np.where(clusterer.labels_ == cluster_index)[0]\n",
    "                memberships[k_index] = other_members_union.astype(int)\n",
    "                \n",
    "        return self.memory_bank.mask(np.array(memberships, dtype=object))\n",
    "    \n",
    "    def _intersecter(self, n1, n2):\n",
    "        return np.array([[v1 and v2 for v1, v2 in zip(n1_x, n2_x)] for n1_x, n2_x in zip(n1, n2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " def _prob_density(self, codes, indices):\n",
    "        '''Compute the unnormalized probability density for the codes being in the sets defined by the indices\n",
    "        \n",
    "        Returns:\n",
    "            prob_dens (Tensor): The unnormalized probability density of the vectors with given codes being part\n",
    "                of the subset of codes specified by the indices. There is one dimension, the batch dimension\n",
    "                \n",
    "        '''\n",
    "        ragged = len(set([np.count_nonzero(ind) for ind in indices])) != 1\n",
    "\n",
    "        # In case the subsets of memory vectors are all of the same size, broadcasting can be used and the\n",
    "        # batch dimension is handled concisely. This will always be true for the k-nearest neighbour density\n",
    "        if not ragged:\n",
    "            vals = torch.tensor([np.compress(ind, self.memory_bank.vectors, axis=0) for ind in indices],\n",
    "                                requires_grad=False)\n",
    "            v_dots = torch.matmul(vals, codes.unsqueeze(-1))\n",
    "            exp_values = torch.exp(torch.div(v_dots, self.temperature))\n",
    "            pdensity = torch.sum(exp_values, dim=1).squeeze(-1)\n",
    "\n",
    "        # Broadcasting not possible if the subsets of memory vectors are of different size, so then manually loop\n",
    "        # over the batch dimension and stack results\n",
    "        else:\n",
    "            xx_container = []\n",
    "            for k_item in range(codes.size(0)):\n",
    "                vals = torch.tensor(np.compress(indices[k_item], self.memory_bank.vectors, axis=0),\n",
    "                                    requires_grad=False)\n",
    "                v_dots_prime = torch.mv(vals, codes[k_item])\n",
    "                exp_values_prime = torch.exp(torch.div(v_dots_prime, self.temperature))\n",
    "                xx_prime = torch.sum(exp_values_prime, dim=0)\n",
    "                xx_container.append(xx_prime)\n",
    "            pdensity = torch.stack(xx_container, dim=0)\n",
    "\n",
    "        return pdensity\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accurate-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prob_density(self, codes, indices):\n",
    "    ragged = len(set([np.count_nonzero(ind) for ind in indices])) != 1          # np.count_nonzero()を使うとTrueの数、すなわち、条件を満たす要素の個数が得られる\n",
    "    if not ragged:\n",
    "        vals = torch.tensor([np.compress(ind, self.memory_bank.vectors, axis=0) for ind in indices],\n",
    "                            requires=False)\n",
    "        v_dots = torch.matmul(vals, codes.unsqueeze(-1))     #torch.matmulとは積を計算するものです。普通に行列計算するだけです\n",
    "        exp_values_prime = torch.exp(torch.div(v_dots, self.temperature))\n",
    "        pdensity = torch.sum(exp_values, dim=1).squeeze(-1)      # squeeze, unsqueeze in detail: https://stackoverflow.com/questions/61598771/pytorch-squeeze-and-unsqueeze\n",
    "        \n",
    "    else:\n",
    "        xx_container =  []\n",
    "        for k_item in range(codes.size(0)):\n",
    "            vals = torch.tensor(np.compress(indices[k_item], self.memory_bank.vectors, axis=0),\n",
    "                                requires_grad=False)\n",
    "            v_dots_prime = torch.mv(vals, codes[k_item])      # torch.dotとtorch.mmとtorch.mvとtorch.bmmとtorch.matmul  in detail: https://qiita.com/tand826/items/9e1b6a4de785097fe6a5\n",
    "            exp_values_prime = torch.exp(torch.div(v_dots_prime, self.temperature))\n",
    "            xx_prime = torch.sum(exp_values_prime, dim=0)\n",
    "            xx_container.append(xx_prime)\n",
    "        pdensity = torch.stack(xx_container, dim=0)\n",
    "        \n",
    "    return pdensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeeze, unsqueeze\n",
    "\"\"\"\n",
    "It is useful for providing single sample to the network (which requires first dimension to be batch), for images it would be\n",
    "\n",
    "例\n",
    "# 3 channels, 32 width, 32 height\n",
    "tensor = torch.randn(3, 32, 32)\n",
    "# 1 batch, 3 channels, 32 width, 32 height\n",
    "tensor.unsqueeze(dim=0).shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Model and Loss Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import fungidata\n",
    "from ae_deep import EncoderVGGMerged\n",
    "from cluster_utils import MemoryBank, LocalAggregationLoss\n",
    "\n",
    "# Create fungi Dataset (details omitted)\n",
    "dataset = fungidata.factory.create('grid basic idx', ...)\n",
    "dataloader = DataLoader(dataset, ...)\n",
    "\n",
    "# Instantiate custom-made model and criterion with initial memory bank from pre-trained VGG-Encoder\n",
    "model = EncoderVGGMerged(merger_type='mean')\n",
    "memory_bank = MemoryBank(n_vectors=5400, dim_vector=model.channels_code, memory_mixing_rate=0.5)\n",
    "memory_bank.vectors = normalize(model.eval_codes_for_(dataloader), axis=1)\n",
    "criterion = LocalAggregationLoss(memory_bank=memory_bank,\n",
    "                                 temperature=0.07, k_nearest_neighbours=500, clustering_repeats=6, number_of_centroids=100)\n",
    "\n",
    "# Instantiate a stochastic-gradient descent optimizer \n",
    "optimizer = SGD(model.parameters())\n",
    "\n",
    "# Rudimentary outline of training loop\n",
    "for epoch in range(20):\n",
    "    for inputs in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs['image'])\n",
    "        loss = criterion(output, inputs['idx'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "governing-closer",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 4 required positional arguments: 'img_input_dim', 'img_n_splits', 'crop_step_size', and 'crop_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-70a8371a1360>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcluster_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMemoryBank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLocalAggregationLoss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfungidata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'grid basic idx'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C:\\\\Users\\\\shota\\\\Documents\\\\Data\\\\Kaggle\\\\lesson_image_data\\\\Intel_Image_Classification\\\\seg_pred\\\\seg_pred'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\fungidata.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, key, csv_file, img_root_dir, selector, iselector, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unregistered data builder: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m         return builder(csv_file=csv_file, img_root_dir=img_root_dir, selector=selector, iselector=iselector,\n\u001b[0m\u001b[0;32m    506\u001b[0m                        **kwargs)\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() missing 4 required positional arguments: 'img_input_dim', 'img_n_splits', 'crop_step_size', and 'crop_dim'"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD        # SGD（確率的勾配降下法）\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import normalize\n",
    "import fungidata\n",
    "from ae_deep import EncoderVGGMerged\n",
    "from cluster_utils import MemoryBank, LocalAggregationLoss\n",
    "\n",
    "dataset = fungidata.factory.create('grid basic idx', 'C:\\\\Users\\\\shota\\\\Documents\\\\Data\\\\Kaggle\\\\lesson_image_data\\\\Intel_Image_Classification\\\\seg_pred\\\\seg_pred')\n",
    "dataloader = DataLoader(dataset, ...)\n",
    "\n",
    "model = EncoderVGGMerged(merger_type='mean')\n",
    "memory_bank = MemoryBank(n_vectors=5400, dim_vector=model.channels_code, memory_mixing_rate=0.5)\n",
    "memory_bank.vectors = normalize(model.eval_codes_for_(dataloader), axis=1)\n",
    "criterion = LocalAggregationLoss(memory_bank=memory_bank,\n",
    "                                 temperature=0.07, k_nearest_nenighbours=500, clustering_repeats=6, number_of_centroids=100)\n",
    "\n",
    "optimizer = SGD(model.parameters())\n",
    "\n",
    "for epoch in range(20):\n",
    "    for inputs in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs['image'])\n",
    "        loss = criterion(output, inputs['idx'])\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-digest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
