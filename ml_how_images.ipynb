{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rental-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sufficient-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for purposes of reproducibility\n",
    "seed = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "balanced-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authorized-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "representative-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the inputs from 0-255 to between 0 and 1 by dividing by 255\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tight-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "#(one-hot encoding is best used when doing binary classification)\n",
    "#One-Hotエンコーディング: One-Hot、つまり1つだけ1でそれ以外は0のベクトル（行列）を指し,One-Hotエンコーディングもダミー変数もやっていることはほとんど同じで、カテゴリー変数を0,1の変数に変換して、学習器が学習しやすい形に変換\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "class_num = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" one-hot-encoding 例\n",
    "X=np.array([0,1,2,1,0])\n",
    "X\n",
    "array([0, 1, 2, 1, 0])\n",
    "変換後\n",
    "np.eye(3)[X]\n",
    "array([[1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 1., 0.],\n",
    "       [1., 0., 0.]])\n",
    "こっちのほうが機械が分かりやすい\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "falling-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "distinct-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As mentioned, relu is the most common activation, and padding='same' just means we aren't changing the size of the image at all\n",
    "#  the number of channels/filters we want (that's the 32 below), the size of the filter we want (3 x 3 in this case), the input shape (when creating the first layer)\n",
    "model.add(Conv2D(32, (3, 3), input_shape=x_train.shape[1:], padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#or\n",
    "#model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "burning-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will make a dropout layer to prevent overfitting, which functions by randomly eliminating some of the connections between the layers (0.2 means it drops 20% of the existing connections)\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inner-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch Normalization normalizes the inputs heading into the next layer, ensuring that the network always creates activations with the same distribution that we desire\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "waiting-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now comes another convolutional layer, but the filter size increases so the network can learn more complex representations\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "processed-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's the pooling layer, as discussed before this helps make the image classifier more robust so it can learn relevant patterns. There's also the dropout and batch normalization\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secondary-netscape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThat's the basic flow for the first half of a CNN implementation:\\nConvolutional, activation, dropout, pooling.\\nYou can now see why we have imported \\nDropout, BatchNormalization, Activation, Conv2d, and MaxPooling2d\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "That's the basic flow for the first half of a CNN implementation:\n",
    "Convolutional, activation, dropout, pooling.\n",
    "You can now see why we have imported \n",
    "Dropout, BatchNormalization, Activation, Conv2d, and MaxPooling2d\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "disciplinary-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The exact number of pooling layers you should use will vary depending on the task you are doing, and it's something you'll get a feel for over time. Since the images are so small here already we won't pool more than twice.\n",
    "#You can now repeat these layers to give your network more representations to work off of\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))      #Conv2D(16, (3, 3)のコードでは、カーネルという入力データにかける「3×3」の16種類のフィルタを各マスにかけていき、16（枚）の出力データを得られるように指定\n",
    "model.add(Activation('relu'))       #Rectified Linear Unit: フィルタ後の画像に実施。入力が0以下の時は出力0。入力が0より大きい場合はそのまま出力\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "studied-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After we are done with the convolutional layers, we need to Flatten the data, which is why we imported the function above. We'll also add a layer of dropout again\n",
    "model.add(Flatten())     #平坦化（次元削減） – 1次元ベクトルに変換する\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "agreed-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we make use of the Dense import and create the first densely connected layer. We need to specify the number of neurons in the dense layer. Note that the numbers of neurons in succeeding layers decreases, eventually approaching the same number of neurons as there are classes in the dataset (in this case 10).\n",
    "#The kernel constraint can regularize the data as it learns, another thing that helps prevent overfitting. This is why we imported maxnorm earlier.\n",
    "model.add(Dense(256, kernel_constraint=maxnorm(3)))  #全結合層。出力256(投票のようなもの)\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sufficient-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, the softmax activation function selects the neuron with the highest probability as its output, voting that the image belongs to that class\n",
    "model.add(Dense(class_num))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "material-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The optimizer is what will tune the weights in your network to approach the point of lowest loss. The Adam algorithm is one of the most commonly used optimizers because it gives great performance on most problems\n",
    "epochs = 25\n",
    "optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "auburn-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's now compile the model with our chosen parameters. Let's also specify a metric to use\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fluid-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,264,458\n",
      "Trainable params: 2,263,114\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# We can print out the model summary to see what the whole model looks like\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "thousand-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 [==============================] - 184s 236ms/step - loss: 1.5236 - accuracy: 0.4589 - val_loss: 1.1552 - val_accuracy: 0.5862\n",
      "Epoch 2/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 1.0416 - accuracy: 0.6299 - val_loss: 0.9113 - val_accuracy: 0.6765\n",
      "Epoch 3/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.8639 - accuracy: 0.6949 - val_loss: 0.8037 - val_accuracy: 0.7160\n",
      "Epoch 4/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.7616 - accuracy: 0.7337 - val_loss: 0.6797 - val_accuracy: 0.7601\n",
      "Epoch 5/25\n",
      "782/782 [==============================] - 183s 234ms/step - loss: 0.7014 - accuracy: 0.7548 - val_loss: 0.6736 - val_accuracy: 0.7652\n",
      "Epoch 6/25\n",
      "782/782 [==============================] - 183s 234ms/step - loss: 0.6613 - accuracy: 0.7672 - val_loss: 0.6360 - val_accuracy: 0.7812\n",
      "Epoch 7/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.6311 - accuracy: 0.7799 - val_loss: 0.6338 - val_accuracy: 0.7824\n",
      "Epoch 8/25\n",
      "782/782 [==============================] - 186s 238ms/step - loss: 0.6052 - accuracy: 0.7870 - val_loss: 0.6510 - val_accuracy: 0.7745\n",
      "Epoch 9/25\n",
      "782/782 [==============================] - 183s 234ms/step - loss: 0.5754 - accuracy: 0.7986 - val_loss: 0.6141 - val_accuracy: 0.7881\n",
      "Epoch 10/25\n",
      "782/782 [==============================] - 183s 234ms/step - loss: 0.5583 - accuracy: 0.8035 - val_loss: 0.5859 - val_accuracy: 0.8011\n",
      "Epoch 11/25\n",
      "782/782 [==============================] - 183s 234ms/step - loss: 0.5333 - accuracy: 0.8129 - val_loss: 0.6585 - val_accuracy: 0.7732\n",
      "Epoch 12/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.5263 - accuracy: 0.8152 - val_loss: 0.5553 - val_accuracy: 0.8104\n",
      "Epoch 13/25\n",
      "782/782 [==============================] - 181s 232ms/step - loss: 0.4995 - accuracy: 0.8242 - val_loss: 0.5799 - val_accuracy: 0.8055\n",
      "Epoch 14/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.4977 - accuracy: 0.8245 - val_loss: 0.5412 - val_accuracy: 0.8191\n",
      "Epoch 15/25\n",
      "782/782 [==============================] - 183s 234ms/step - loss: 0.4838 - accuracy: 0.8316 - val_loss: 0.5724 - val_accuracy: 0.8060\n",
      "Epoch 16/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.4767 - accuracy: 0.8338 - val_loss: 0.5415 - val_accuracy: 0.8157\n",
      "Epoch 17/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.4695 - accuracy: 0.8358 - val_loss: 0.5498 - val_accuracy: 0.8134\n",
      "Epoch 18/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.4540 - accuracy: 0.8423 - val_loss: 0.5292 - val_accuracy: 0.8202\n",
      "Epoch 19/25\n",
      "782/782 [==============================] - 185s 236ms/step - loss: 0.4487 - accuracy: 0.8425 - val_loss: 0.5514 - val_accuracy: 0.8153\n",
      "Epoch 20/25\n",
      "782/782 [==============================] - 183s 234ms/step - loss: 0.4458 - accuracy: 0.8442 - val_loss: 0.5654 - val_accuracy: 0.8142\n",
      "Epoch 21/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.4356 - accuracy: 0.8471 - val_loss: 0.5209 - val_accuracy: 0.8224\n",
      "Epoch 22/25\n",
      "782/782 [==============================] - 183s 234ms/step - loss: 0.4344 - accuracy: 0.8486 - val_loss: 0.5047 - val_accuracy: 0.8312\n",
      "Epoch 23/25\n",
      "782/782 [==============================] - 184s 235ms/step - loss: 0.4279 - accuracy: 0.8505 - val_loss: 0.5177 - val_accuracy: 0.8232\n",
      "Epoch 24/25\n",
      "782/782 [==============================] - 183s 235ms/step - loss: 0.4204 - accuracy: 0.8513 - val_loss: 0.5325 - val_accuracy: 0.8221\n",
      "Epoch 25/25\n",
      "782/782 [==============================] - 182s 233ms/step - loss: 0.4208 - accuracy: 0.8513 - val_loss: 0.5311 - val_accuracy: 0.8216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13f08d10100>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we get to training the model. To do this, all we have to do is call the fit() function on the model and pass in the chosen parameters.\n",
    "#Here's where I use the seed I chose, for the purposes of reproducibility\n",
    "np.random.seed(seed)\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=64)\n",
    "\n",
    "#We'll be training on 50000 samples and validating on 10000 samples.\n",
    "#Running this piece of code will yield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "developmental-organic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.16%\n"
     ]
    }
   ],
   "source": [
    "#Now we can evaluate the model and see how it performed. Just call model.evaluate():\n",
    "# Model evaluation\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-probe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
