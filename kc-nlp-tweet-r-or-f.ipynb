{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://www.kaggle.com/moeinshariatnia/simple-distilbert-fine-tuning-0-84-lb# \nimport os\nimport copy\nimport pandas as pd\nimport numpy as np\nfrom tqdm.autonotebook import tqdm     # プログレスバーを表示\n\nimport torch\nimport torch.nn as nn\n\n\nfrom sklearn.model_selection import train_test_split, KFold\n\n# importing HuggingFace transformers library which is all we need to get SOTA results :)\nimport transformers\nfrom transformers import get_linear_schedule_with_warmup\n\nprint(transformers.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building A Custom PyTorch Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, tokenizer, mode='train', max_length=None):\n        self.dataframe = dataframe\n        if mode != 'test':\n            self.targets = dataframe['target'].values\n        texts = list(dataframe['text'].values)\n        self.encodings = tokenizer(texts,\n                                   padding=True,\n                                   truncation=True,\n                                   max_length=max_length)\n        self.mode = mode\n        \n    def __getitem__(self, idx):\n        # putting each tensor in front of the corresponding key from the tokenizer\n        # HuggingFace tokenizers give you whatever you need to feed to the corresponding model\n        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n        # when testing, there are no targets so we won't do the following\n        \n        if self.mode != 'test':\n            item['labels'] = torch.tensor(self.targets[idx])\n        return item\n    \n    def __len__(self):\n        return len(self.dataframe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_loaders(dataframe, tokenizer, mode='train', max_length=None):\n    dataset = TweetDataset(dataframe, tokenizer, mode, max_length=max_length)\n    dataloader = torch.utils.data.DataLoader(dataset,\n                                             batch_size=options.batch_size,\n                                             shuffle=True if mode == 'train' else False,\n                                             num_workers=options.num_workers)\n    return dataloader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Custom Classification Model based on DistilBERT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self,\n                 bert_model,\n                 num_labels,\n                 bert_hidden_dim=768,\n                 classifier_hidden_dim=768,\n                 dropout=None):\n        \n        super().__init__()\n        self.bert_model = bert_model\n        # nn.Identity does nothing if the dropout is set to None\n        self.head = nn.Sequential(nn.Linear(bert_hidden_dim, classifier_hidden_dim),\n                                  nn.ReLU(),\n                                  nn.Dropout(dropout) if dropout is not None else nn.Identity(),\n                                  nn.Linear(classifier_hidden_dim, num_labels))\n        \n    def forward(self, batch):\n        # feeding the input_ids and masks to the model. These are provided by our tokenizer\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        # obtaining the last layer hidden states of the Transformer\n        last_hidden_state = output.last_hidden_state   # shape: (batch_size, seq_length, bert_hidden_dim)\n        # As I said, the CLS token is in the beginning of the sequence. So, we grab its representation \n        # by indexing the tensor containing the hidden representations\n        CLS_token_state = last_hidden_state[:, 0, :]\n        # passing this representation through our custom head\n        logits = self.head(CLS_token_state)\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training and Evaluation functions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AvgMeter:\n    def __init__(self, name='Metric'):\n        self.name = name\n        self.reset()\n        \n    def reset(self):\n        self.avg, self.sum, self.count = [0]*3\n        \n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val*count\n        self.avg = self.sum / self.count\n        \n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text\n    \ndef one_epoch(model, criterion, loader, device, optimizer=None, lr_scheduler=None, mode='train', step='batch'):\n    loss_meter = AvgMeter()\n    acc_meter = AvgMeter()\n    \n    tqdm_object = tqdm(loader, total=len(loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        preds = model(batch)\n        loss  = criterion(preds, batch['labels'])\n        if mode == 'train':\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if step == 'batch':\n                lr_scheduler.step()\n                \n        count = batch['input_ids'].size(0)\n        loss_meter.update(loss.item(), count)\n        \n        accuracy = get_accuracy(preds.detach(), batch['labels'])\n        acc_meter.update(accuracy.item(), count)\n        if mode == 'train':\n            tqdm_object.set_postfix(loss=loss_meter.avg, accuracy=acc_meter.avg, lr=get_lr(optimizer))\n        else:\n            tqdm_object.set_postfix(loss=loss_meter.avg, accuracy=acc_meter.avg)\n            \n    return loss_meter, acc_meter\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n    \ndef get_accuracy(preds, targets):\n    \"\"\"\n    preds shape: (batch_size, num_labels)\n    targets shape: (batch_size)\n    \"\"\"\n    preds = preds.argmax(dim=1)\n    acc = (preds == targets).float().mean()\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_eval(epochs, model, train_loader, valid_loader, criterion, optimizer, device, options, lr_scheduler=None):\n    best_loss = float('inf')\n    best_model_weights = copy.deepcopy(model.state_dict())\n    \n    for epoch in range(epochs):\n        print('*'*30)\n        print(f\"Epoch {epoch + 1}\")\n        current_lr = get_lr(optimizer)\n        \n        model.train()\n        train_loss, train_acc = one_epoch(model,\n                                          criterion,\n                                          train_loader,\n                                          device,\n                                          optimizer=optimizer,\n                                          lr_scheduler=lr_scheduler,\n                                          mode='train',\n                                          step=options.step)\n        model.eval()\n        with torch.no_grad():\n            valid_loss, valid_acc = one_epoch(model,\n                                              criterion,\n                                              valid_loader,\n                                              device,\n                                              optimizer=None,\n                                              lr_scheduler=None,\n                                              mode='valid')\n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            best_model_weights = copy.deepcopy(model.state_dict())\n            torch.save(model.state_dict(), f'{options.model_path}/{options.model_save_name}')\n            print('Saved best model!')\n            \n        # or you could do: if step == \"epoch\":\n        if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            lr_scheduler.step(valid_loss.avg)\n            # if the learning rate changes by ReduceLROnPlateau, we are going to\n            # reload our previous best model weights and start from there with a lower LR\n            if current_lr != get_lr(optimizer):\n                print(\"Loading best model \")\n                model.load_state_dict(torch.load(f'{options.model_path}/{options.model_save_name}', map_location=device))\n                \n        print(f\"Train loss: {train_loss.avg:.5f}\")\n        print(f\"Train accracy: {train_acc.avg:.5f}\")\n        \n        print(f\"Valid loss: {valid_loss.avg:.5f}\")\n        print(f\"Valid accuracy: {valid_acc.avg:.5f}\")\n        print('*'*30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Options:\n    model_name = 'distilbert-base-uncased'\n    batch_size = 64\n    num_labels = 2\n    epochs = 10\n    num_workers = 2\n    learning_rate = 3e-5\n    scheduler = 'ReduceLROnPlateau'\n    patience = 2\n    dropout = 0.5\n    model_path = '/kaggle/working'\n    max_length = 140\n    model_save_name = 'model.pt'\n    n_folds = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking care of Cross Validation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_folds(dataframe, n_splits=5):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    for i, (_, valid_idx) in enumerate(kf.split(X=dataframe['id'])):\n        dataframe.loc[valid_idx, 'fold']  = i\n    return dataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_fold(fold, options):\n    print(f\"Training Fold: {fold}\")\n    \n     # Here, we load the pre-trained DistilBERT model from transformers library\n    bert_model = transformers.DistilBertModel.from_pretrained(options.model_name)\n    # Loading the corresponding tokenizer from HuggingFace by using AutoTokenizer class.\n    tokenizer = transformers.AutoTokenizer.from_pretrained(options.model_name, use_fast=True)\n    \n    dataframe = pd.read_csv('../input/nlp-getting-started/train.csv')\n    dataframe = make_folds(dataframe, n_splits=options.n_folds)\n    train_dataframe = dataframe[dataframe['fold'] != fold].reset_index(drop=True)\n    valid_dataframe = dataframe[dataframe['fold'] == fold].reset_index(drop=True)\n    \n    train_loader = make_loaders(train_dataframe, tokenizer, \"train\", options.max_length)\n    valid_loader = make_loaders(valid_dataframe, tokenizer, \"valid\", options.max_length)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = CustomModel(bert_model, options.num_labels, dropout=options.dropout).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=options.learning_rate)\n    if options.scheduler == \"ReduceLROnPlateau\":\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                                  mode = 'min',\n                                                                  factor = 0.5,\n                                                                  patience = options.patience)\n        \n        # when to step the scheduler: after an epoch or after a batch\n        options.step = 'epoch'\n        \n    elif options.scheduler == \"LinearWarmup\":\n        num_train_steps = len(train_loader) * options.epochs\n        lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = num_train_steps)\n        \n        # when to step the scheduler: after an epoch or after a batch\n        options.step = 'batch'\n        \n    criterion = nn.CrossEntropyLoss()\n    options.model_save_name = f\"model_fold_{fold}.pt\"\n    train_eval(options.epochs, model, train_loader, valid_loader,\n               criterion, optimizer, device, options, lr_scheduler=lr_scheduler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_folds(options):\n    n_folds = options.n_folds\n    for i in range(n_folds):\n        one_fold(fold=i, options=options)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"options = Options()\ntrain_folds(options)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_one_model(options):\n    test_dataframe = pd.read_csv('../input/nlp-getting-started/test.csv')\n    bert_model = transformers.DistilBertModel.from_pretrained(options.model_name)\n    tokenizer = transformers.AutoTokenizer.from_pretrained(options.model_name, use_fast=True)\n    \n    test_loader = make_loaders(test_dataframe, tokenizer, mode='test')\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = CustomModel(bert_model, options.num_labels, dropout=options.dropout).to(device)\n    model.load_state_dict(torch.load(f\"{options.model_path}/{options.model_save_name}\", map_location=device))   # torch.load_state_dict: 読み込み\n    model.eval()\n    \n    all_preds = None\n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            preds = model(batch)\n            if all_preds is None:\n                all_preds = preds\n            else:\n                all_preds = torch.cat([all_preds, preds], dim=0)     # torch,cat: 連結\n                \n    return all_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_all_models(options):\n    n_folds = options.n_folds\n    all_model_preds = []\n    for fold in range(n_folds):\n        options.model_save_name = f\"model_fold_{fold}.pt\"\n        all_preds = test_one_model(options)\n        all_model_preds.append(all_preds)\n        \n    all_model_preds = torch.stack(all_model_preds, dim=0)\n    print(all_model_preds.shape)\n     # I will return the mean of the final predictions of all the models\n    # You could do other things like 'voting' between the five models\n    return all_model_preds.mean(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds = test_all_models(options)\npredictions = all_preds.argmax(dim=1).cpu().numpy()   #argmax: 一番大きい要素のインデックス(位置)を表示\nsample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsample_submission['target'] = predictions\nsample_submission.to_csv('sample_submission.csv', index=False)\npd.read_csv('sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}